{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Scraping https://googleapis.github.io/python-genai/index.html\n",
      "🔍 Scraping https://googleapis.github.io/python-genai/genai.html\n",
      "🔍 Scraping https://googleapis.github.io/python-genai/_sources/index.rst.txt\n",
      "🔍 Scraping https://googleapis.github.io/python-genai/_sources/genai.rst.txt\n",
      "✅ 全部完成！\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import sqlite3\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ====== 參數設定 ======\n",
    "BASE_URL   = \"https://googleapis.github.io/python-genai/\"\n",
    "START_PAGE = \"index.html\"\n",
    "OUTPUT_DIR = \"python_genai_docs\"\n",
    "DB_PATH    = \"python_genai_docs.db\"\n",
    "\n",
    "# 建立輸出資料夾\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 建立 SQLite 資料庫 & table\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cur  = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS pages (\n",
    "    url        TEXT PRIMARY KEY,\n",
    "    title      TEXT,\n",
    "    file_path  TEXT,\n",
    "    scraped_at TEXT\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "session = requests.Session()\n",
    "visited  = set()\n",
    "to_visit = [urljoin(BASE_URL, START_PAGE)]\n",
    "\n",
    "def is_valid_url(url: str) -> bool:\n",
    "    \"\"\"\n",
    "    檢查 URL 是否屬於我們要爬的範圍：\n",
    "    - 同一網域、同一路徑前綴\n",
    "    - 剔除靜態資源 (.css/.js/.png...)\n",
    "    \"\"\"\n",
    "    p = urlparse(url)\n",
    "    b = urlparse(BASE_URL)\n",
    "    if p.netloc != b.netloc: return False\n",
    "    if not p.path.startswith(b.path): return False\n",
    "    if any(p.path.endswith(ext) for ext in (\".css\",\".js\",\".png\",\".jpg\",\n",
    "                                            \".svg\",\".gif\",\".ico\")):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# ====== 開始爬取 ======\n",
    "while to_visit:\n",
    "    url = to_visit.pop(0)\n",
    "    if url in visited:\n",
    "        continue\n",
    "    visited.add(url)\n",
    "    print(f\"🔍 Scraping {url}\")\n",
    "\n",
    "    resp = session.get(url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # 取第一個 <h1> 或 <title> 當標題\n",
    "    title_tag = soup.find(\"h1\") or soup.find(\"title\")\n",
    "    title     = title_tag.get_text(strip=True) if title_tag else url\n",
    "\n",
    "    # 擷取主要內容\n",
    "    container = (soup.find(\"main\")\n",
    "                 or soup.find(attrs={\"role\":\"main\"})\n",
    "                 or soup.body)\n",
    "    markdown = md(str(container), heading_style=\"ATX\")\n",
    "\n",
    "    # 產生安全的檔名\n",
    "    safe = \"\".join(c if c.isalnum() or c in \"-_\" else \"_\" \n",
    "                   for c in title.lower())[:50]\n",
    "    fname = f\"{safe}.md\"\n",
    "    fpath = os.path.join(OUTPUT_DIR, fname)\n",
    "\n",
    "    # 寫入 .md\n",
    "    with open(fpath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"# {title}\\n\\n{markdown}\")\n",
    "\n",
    "    # 存入資料庫\n",
    "    scraped_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
    "    cur.execute(\"\"\"\n",
    "    INSERT OR REPLACE INTO pages (url, title, file_path, scraped_at)\n",
    "    VALUES (?, ?, ?, ?)\n",
    "    \"\"\", (url, title, fpath, scraped_time))\n",
    "    conn.commit()\n",
    "\n",
    "    # 掃描頁面中所有連結，排入待抓清單\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].split(\"#\")[0]  # 去除錨點\n",
    "        if not href:\n",
    "            continue\n",
    "        new_url = urljoin(url, href)\n",
    "        if is_valid_url(new_url) and new_url not in visited and new_url not in to_visit:\n",
    "            to_visit.append(new_url)\n",
    "\n",
    "conn.close()\n",
    "print(\"✅ 全部完成！\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
