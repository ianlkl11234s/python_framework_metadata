{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”µ å´æ¬„å·²å®Œå…¨å±•é–‹\n",
      "ğŸ”µ å…±æ‰¾åˆ° 0 å€‹å°é é¢ï¼Œé–‹å§‹çˆ¬å–...\n",
      "ğŸ‰ å…¨éƒ¨å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "BASE_URL = \"https://docs.streamlit.io/develop\"\n",
    "SAVE_ROOT = \"/Users/migu/Desktop/è³‡æ–™åº«/gen_ai_try/ai_metadata/streamlit\"\n",
    "CRAWL_DELAY = 1.5\n",
    "\n",
    "async def scrape_main_content_as_markdown(url, save_path):\n",
    "    \"\"\"å¾å–®ä¸€é é¢çˆ¬ä¸»å…§æ–‡ï¼Œè½‰æˆ markdown å„²å­˜\"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    markdown_lines = [\n",
    "        \"---\",\n",
    "        f\"title: Streamlit Documentation Capture\",\n",
    "        f\"url: {url}\",\n",
    "        f\"date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        \"---\\n\"\n",
    "    ]\n",
    "\n",
    "    main = soup.find('main')\n",
    "    if not main:\n",
    "        print(f\"âš ï¸ æ‰¾ä¸åˆ°ä¸»å…§å®¹: {url}\")\n",
    "        return\n",
    "\n",
    "    for el in main.find_all(['h1','h2','h3','h4','h5','h6','p','pre','code','ul','ol','li']):\n",
    "        if el.name.startswith('h'):\n",
    "            lvl = int(el.name[1])\n",
    "            markdown_lines.append(f\"{'#'*lvl} {el.get_text(strip=True)}\\n\")\n",
    "        elif el.name == 'p':\n",
    "            text = el.get_text(strip=True)\n",
    "            if text:\n",
    "                markdown_lines.append(text + \"\\n\")\n",
    "        elif el.name == 'pre':\n",
    "            code = el.get_text()\n",
    "            markdown_lines.append(f\"```python\\n{code}\\n```\\n\")\n",
    "        elif el.name == 'code':\n",
    "            inline = el.get_text(strip=True)\n",
    "            if inline:\n",
    "                markdown_lines.append(f\"`{inline}`\\n\")\n",
    "        elif el.name in ['ul','ol']:\n",
    "            for li in el.find_all('li'):\n",
    "                li_txt = li.get_text(strip=True)\n",
    "                markdown_lines.append(f\"- {li_txt}\")\n",
    "\n",
    "    content = \"\\n\".join(markdown_lines)\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    print(f\"âœ… å·²å®Œæˆ: {save_path}\")\n",
    "\n",
    "async def expand_all_details(page):\n",
    "    \"\"\"å±•é–‹æ‰€æœ‰ <details> æ¨™ç±¤ä¸‹å°šæœªæ‰“é–‹çš„å´æ¬„ç¯€é»\"\"\"\n",
    "    while True:\n",
    "        details = await page.query_selector_all(\"nav details:not([open])\")\n",
    "        if not details:\n",
    "            break\n",
    "        print(f\"ğŸ”µ å°šæœ‰ {len(details)} å€‹ç¯€é»æœªå±•é–‹ï¼Œæ­£åœ¨å±•é–‹...\")\n",
    "        for d in details:\n",
    "            try:\n",
    "                summ = await d.query_selector(\"summary\")\n",
    "                if summ:\n",
    "                    await summ.click()\n",
    "                    await page.wait_for_timeout(300)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ å±•é–‹å¤±æ•—: {e}\")\n",
    "\n",
    "async def get_target_links(page):\n",
    "    \"\"\"åœ¨å´æ¬„å±•é–‹å®Œç•¢å¾Œï¼Œæ’ˆå‡ºæ‰€æœ‰ /develop/xxx/yyy/zzz æ ¼å¼çš„é€£çµ\"\"\"\n",
    "    links = set()\n",
    "    a_tags = await page.query_selector_all(\"nav a.menu__link\")\n",
    "    for a in a_tags:\n",
    "        href = await a.get_attribute(\"href\")\n",
    "        if href and href.startswith(\"/develop\"):\n",
    "            parts = urlparse(href).path.strip(\"/\").split(\"/\")\n",
    "            if len(parts) == 4:\n",
    "                full = urljoin(BASE_URL, href)\n",
    "                links.add(full)\n",
    "    return list(links)\n",
    "\n",
    "async def main():\n",
    "    # 1. å•Ÿå‹•ä¸¦å±•é–‹å´æ¬„\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(BASE_URL, wait_until=\"networkidle\")\n",
    "        await page.wait_for_selector(\"nav\")\n",
    "        await expand_all_details(page)\n",
    "        print(\"ğŸ”µ å´æ¬„å·²å®Œå…¨å±•é–‹\")\n",
    "\n",
    "        # 2. æŠ“å–æ‰€æœ‰ç›®æ¨™å°é é€£çµ\n",
    "        links = await get_target_links(page)\n",
    "        print(f\"ğŸ”µ å…±æ‰¾åˆ° {len(links)} å€‹å°é é¢ï¼Œé–‹å§‹çˆ¬å–...\")\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    # 3. é€é çˆ¬å–ä¸¦å­˜æª”\n",
    "    for idx, url in enumerate(links):\n",
    "        path = urlparse(url).path.lstrip(\"/\")\n",
    "        save_path = os.path.join(SAVE_ROOT, f\"{path}.md\")\n",
    "        try:\n",
    "            await scrape_main_content_as_markdown(url, save_path)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ çˆ¬å–å¤±æ•—: {url} - {e}\")\n",
    "        if idx < len(links) - 1:\n",
    "            await asyncio.sleep(CRAWL_DELAY)\n",
    "\n",
    "    print(\"ğŸ‰ å…¨éƒ¨å®Œæˆï¼\")\n",
    "\n",
    "# è‹¥åœ¨ Notebook/Colab åŸ·è¡Œï¼š\n",
    "await main()\n",
    "# è‹¥åœ¨ .py è…³æœ¬åŸ·è¡Œï¼Œè«‹æ”¹ç‚ºï¼š\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
