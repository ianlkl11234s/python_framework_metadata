{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 側欄已完全展開\n",
      "🔵 共找到 0 個小頁面，開始爬取...\n",
      "🎉 全部完成！\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "BASE_URL = \"https://docs.streamlit.io/develop\"\n",
    "SAVE_ROOT = \"/Users/migu/Desktop/資料庫/gen_ai_try/ai_metadata/streamlit\"\n",
    "CRAWL_DELAY = 1.5\n",
    "\n",
    "async def scrape_main_content_as_markdown(url, save_path):\n",
    "    \"\"\"從單一頁面爬主內文，轉成 markdown 儲存\"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    markdown_lines = [\n",
    "        \"---\",\n",
    "        f\"title: Streamlit Documentation Capture\",\n",
    "        f\"url: {url}\",\n",
    "        f\"date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        \"---\\n\"\n",
    "    ]\n",
    "\n",
    "    main = soup.find('main')\n",
    "    if not main:\n",
    "        print(f\"⚠️ 找不到主內容: {url}\")\n",
    "        return\n",
    "\n",
    "    for el in main.find_all(['h1','h2','h3','h4','h5','h6','p','pre','code','ul','ol','li']):\n",
    "        if el.name.startswith('h'):\n",
    "            lvl = int(el.name[1])\n",
    "            markdown_lines.append(f\"{'#'*lvl} {el.get_text(strip=True)}\\n\")\n",
    "        elif el.name == 'p':\n",
    "            text = el.get_text(strip=True)\n",
    "            if text:\n",
    "                markdown_lines.append(text + \"\\n\")\n",
    "        elif el.name == 'pre':\n",
    "            code = el.get_text()\n",
    "            markdown_lines.append(f\"```python\\n{code}\\n```\\n\")\n",
    "        elif el.name == 'code':\n",
    "            inline = el.get_text(strip=True)\n",
    "            if inline:\n",
    "                markdown_lines.append(f\"`{inline}`\\n\")\n",
    "        elif el.name in ['ul','ol']:\n",
    "            for li in el.find_all('li'):\n",
    "                li_txt = li.get_text(strip=True)\n",
    "                markdown_lines.append(f\"- {li_txt}\")\n",
    "\n",
    "    content = \"\\n\".join(markdown_lines)\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    print(f\"✅ 已完成: {save_path}\")\n",
    "\n",
    "async def expand_all_details(page):\n",
    "    \"\"\"展開所有 <details> 標籤下尚未打開的側欄節點\"\"\"\n",
    "    while True:\n",
    "        details = await page.query_selector_all(\"nav details:not([open])\")\n",
    "        if not details:\n",
    "            break\n",
    "        print(f\"🔵 尚有 {len(details)} 個節點未展開，正在展開...\")\n",
    "        for d in details:\n",
    "            try:\n",
    "                summ = await d.query_selector(\"summary\")\n",
    "                if summ:\n",
    "                    await summ.click()\n",
    "                    await page.wait_for_timeout(300)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ 展開失敗: {e}\")\n",
    "\n",
    "async def get_target_links(page):\n",
    "    \"\"\"在側欄展開完畢後，撈出所有 /develop/xxx/yyy/zzz 格式的連結\"\"\"\n",
    "    links = set()\n",
    "    a_tags = await page.query_selector_all(\"nav a.menu__link\")\n",
    "    for a in a_tags:\n",
    "        href = await a.get_attribute(\"href\")\n",
    "        if href and href.startswith(\"/develop\"):\n",
    "            parts = urlparse(href).path.strip(\"/\").split(\"/\")\n",
    "            if len(parts) == 4:\n",
    "                full = urljoin(BASE_URL, href)\n",
    "                links.add(full)\n",
    "    return list(links)\n",
    "\n",
    "async def main():\n",
    "    # 1. 啟動並展開側欄\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(BASE_URL, wait_until=\"networkidle\")\n",
    "        await page.wait_for_selector(\"nav\")\n",
    "        await expand_all_details(page)\n",
    "        print(\"🔵 側欄已完全展開\")\n",
    "\n",
    "        # 2. 抓取所有目標小頁連結\n",
    "        links = await get_target_links(page)\n",
    "        print(f\"🔵 共找到 {len(links)} 個小頁面，開始爬取...\")\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    # 3. 逐頁爬取並存檔\n",
    "    for idx, url in enumerate(links):\n",
    "        path = urlparse(url).path.lstrip(\"/\")\n",
    "        save_path = os.path.join(SAVE_ROOT, f\"{path}.md\")\n",
    "        try:\n",
    "            await scrape_main_content_as_markdown(url, save_path)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 爬取失敗: {url} - {e}\")\n",
    "        if idx < len(links) - 1:\n",
    "            await asyncio.sleep(CRAWL_DELAY)\n",
    "\n",
    "    print(\"🎉 全部完成！\")\n",
    "\n",
    "# 若在 Notebook/Colab 執行：\n",
    "await main()\n",
    "# 若在 .py 腳本執行，請改為：\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
